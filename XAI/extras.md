> Alice and Bob are driving along a road in the city. The sky was dark and visibility was poor. Flashes of lightning occasionally lit up the foreboding cityscape, painting sawtooth patterns along the road ahead. The staccato of raindrops on the windshield gradually turned into a lullaby. Suddenly, a few meters ahead, an irregular shadow stepped into view, jolting Alice into full alertness. On reflex, Alice spins the wheel around and narrowly avoids the shadow. The wheels skidded along the wet road and for a moment the car spins out of control, crashing headfirst into the sidewalk. Right behind Alice in another car, Bob continued on without pause and the irregular shadow dissipated into a mist of water as the car drove over a puddle. So sets the scene for our hypothetical parable.

# Alice and Bob

In some not-so-distant future where our drivers could be human or AI models, our imagined story of Alice and Bob would take on new meaning depending on the nature of their identities. Let us consider two permutations of identities for Alice and Bob, firstly, Alice is an AI and Bob is human; secondly, Alice is human and Bob is an AI. For the cases of an AI driver, we shall further entertain the situation wherein the car has human occupants.

Let us first set the scene formally from each perspective:

- Alice (avoidance case)
  - Observed an obstacle in front of car
  - Decided to avoid said obstacle
  - Successfully avoided obstacle
  - As a result, crashed own car
- Bob (collision case)
  - Failed to detect obstacle in front of car
  - Consequently, did not avoid potential obstacle

This thought experiment examines a probable outcome of employing AI agents in a realistic near-term setting and considers the myraid of complicating factors taken into (or omitted from) consideration in each line of decision. There are a number of factors to consider here:

- Whether there is an obstacle
- Whether an obstacle was observed (whether there really was an obstacle is irrelevant for this point)
- Whether the decision to avoid the obstacle was made (success of avoidance is not relevant here)

We will focus our discussion on two subjects for clarificatory explanation in this example: 1) obstacle detection process, 2) avoidance decision process. 

### Alice AI

Our Alice AI firstly made an incorrect obstacle detection decision since there wasn't actually any obstacle. Secondly, the decision to avoid this imaginary obstacle resulted in harm for the human occupant. What sort of questions should we ask and how do XAI explanations help? Firstly, the failure to make a veridical (correct) obstacle detection judgement is just the sort of panda-gibbon type failure of image classification algorithms. To this failure, we can employ the full force of XAI methods currently under study and examine the possible mechanistic cause of failure and employ explanations of the form of identifying input features contributing to this incorrect determination in an effort to improve our AI's obstacle detection model. Secondly, we would question the decision to swerve in adverse conditions which resulted in harm for the occupants of the car. From a consequentialist perspective, unnecessary harm was done to the occupants of the car, is the AI wrong in its judgement?

Whether this decision is wrong is two-fold, firstly, explanations of the XAI form would help in determining the correct *functioning* of the AI model. We first have to determine that the model is functioning as designed. Secondly, pitting the risk of harming its own occupants against harming a possibly human obstacle will likely have mechanistic explanations available via examining the processes within this decision model as well. However, the XAI type explanations are of no help here in resolving our questions from this front. Comparing against a human Alice driver that makes a similar determination, one would likely praise their selfless action as crucially, we are amenable to the claim that humans have agency. When this agency is placed into the figurative hands of an AI model, it is unclear whether we can make a similar claim. For such ethical questioning, explanations that leave humans out of the loop (by focusing on mechanistic details of models) are next to useless (until such time as we are comfortable with assigning moral status to AI agents). However, we do need to start from establishing facts of the matter before launching into an ethical discourse on the matter (which is the extent to which XAI explanations can help with).

### Bob AI

With our Bob AI, it makes a correct obstacle detection decision and 

### Responsibility of employing AI Agents
