## Singular Learning Theory - implications on neural network robustness

Singular Learning Theory (SLT) proposes that the effective measure of a Neural Network’s complexity is not merely a function of the number of parameters, but needs to account for the geometry of the model’s log likelihood. Real Log Canonical Threshold (RLCT) should be used to assess singular models such as Neural Networks instead of the traditionally employed Bayesian Information Criterion (BIC) which will typically overestimate the complexity of a neural network model.

The authors of SLT first show how the measure of BIC can be derived for regular statistical models and that Neural Networks may violate this regularity prerequisite by having regions of singularity where changing weights result in no change to loss at that local minima. This motivates the need for a better measure of model complexity especially with the phenomena of larger and larger models now having better generalization.

By modelling neural networks as a bayesian optimization problem, the authors note a similarity in the integral of model evidence to the form of Free Energy in physics. This insight prompts them to reflect on how the prior body of work in theoretical physics on singularities can be brought to bear on better specifying the geometry of the likelihood in Neural Network models. Of particular interest is Hironaka's Resolution of Singularities which allows for an analytical treatment of geometry around singularities (such as those around local minima in neural network loss function landscape).

The intuition here for model explainability arising from SLT is that what needs to be explained mainly lies along these singularities, learning (gradient descent/loss propagation/optimization) targets the convergence of a model to a region near such singularities. Having robust constraints/descriptions of the geometry of such singularities for a model would potentially allow us to make broad generalizations of model behavior. Importantly, this goes beyond mapping input features to outputs as most current model explainability methods provide (e.g. saliency maps, LIME).

An interesting conclusion I will note from the results of SLT is that the previous adversarial model attacks for image classifiers may not be unexpected (such as the panda-gibbon example: https://openai.com/index/attacking-machine-learning-with-adversarial-examples/). Manipulations upon the input space allows for travel along regions of singularity where perturbations do not affect model loss at all. Perhaps what we ideally want is the regular case of a single point of optimality, if so, an improvement to making models more robust could be an optimization function that is inversely proportional to the volume around singularities as discerned by the methods of SLT herein, which reduces the possible space of potential unintended adversarial examples.
