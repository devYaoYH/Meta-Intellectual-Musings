## Impact of Choice-set mis-specification on reward learning

Rachel provides a framework to think about the different scenarios under which human feedback is provided to an agent for reward learning[^1]. Crucially, some domains of choice-set mis-alignment can lead to harmful results for reward learning convergence.

Different feedback scenarios are organized along a $c^*$ dimension and the degree of overlap between $C_R$ and $C_H$. The following breaks down the framing cases Rachel introduces.

Cases with the heading A represent when the optimal choice is available to both human and robot:

- A1 → human choice set includes robots fully (ideal case)
- A2 → human choices is subset of robot (cannot provide complete preference alignment, but can provide feedback on $c^*$)
- A3 → A2 + some human choices are unavailable to robot (noisy)

Cases with the heading B represent when the optimal choice is available to the robot, but unavailable to the human:

- B2 → human choices is subset of robot (cannot provide feedback on $c^*$)
- B3 → B2 + some human choices are unavailable to robot (noisy)

In the study, a case that is not considered (since the result cannot be measured alongside the other options) is when the optimal choice is only available to the human and not the robot agent.

Crucially, the study found that for the case B3, the reward parameters that the robot learns is opposite that of the true values, implying a strongly negative effect of the alignment procedure. This is particularly worrying for AI systems with greater capabilities than humans. Where AI systems are particularly helpful could be when it is tasked to find a more optimal policy than that which humans can; and potentially perform actions outside the bounds of human capabilities. This entails that we may not always be able to provide feedback on the trajectories generated by the agent, including the optimal ones. Coupled with the all-too-human nature of inefficiencies (adding to noise where the human choice-set is larger), this squarely falls into the bucket of type B3 reward learning.

The question is whether the agent is able to reflexively identify that it is operating within the context of each reward learning situation and adjust accordingly. For example, by automatically surfacing estimated $c^*$ (initially outside $C_H$) for human feedback, filtering out noise in $C_H$ but not in $C_R$, and report their degree of confidence in estimating the true reward. I think that for purposes of reward learning, it is all the more crucial to keep humans in-the-loop to review the final ‘optimal’ policy that the agents find to avoid mistakes of the sort reported by this paper in type B3.

[^1]: Choice Set Misspecification in Reward Inference, https://arxiv.org/pdf/2101.07691
