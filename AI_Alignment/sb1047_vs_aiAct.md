# SB-1047 vs EU AI Act - two approaches to AI regulation

In California governor Newsom’s rejection of SB-1047, he cites a lack of specific “demonstrable risks” to constrain the set of ‘covered models’. SB-1047 would have enabled a broad swath of current foundational model development to fall under the notion of ‘covered models’ based on their immense resource requirements, *irrespective of their application contexts*. This is in stark contrast to the EU AI Act which explicitly applies to “high-risk AI systems” according to a graded scale of risk assessment (unacceptable, high-risk, limited-risk, minimal-risk)[^1].

I think the two pieces of legislation differ primarily in their targeting of different risks of AI systems. Where the EU AI Act targets somewhat predictable and foreseen risks of deploying AIs within certain applications, SB-1047 attempts to preempt and mitigate unforeseen risks of developing those AI models in the first place.

SB-1047 defines the scope of a ‘covered model’ purely in terms of the dollar cost for model fine-tuning or training. This is likely a proxy for model capabilities given present scaling laws of deep learning. The aim here appears to be avoiding unconstrained widening of AI capability without oversight, or at least some accountability in justifying such risks. The sort of risks legislators have in mind are of the kind listed under ‘critical harms’ which are “caused or materially enabled by” such covered models: Cybersecurity attacks, Chemical, Biological Radiological, Nuclear weapon proliferation, death, gross negligence (to name a few)[^2]. However, the way in which models or conditions conducive to result in such outcomes are not sketched out within the bill. As such, at best, it is a speculative assumption that model capability translates to an increased risk for such undesirable outcomes.

The literature one may draw upon to justify the speculative nature of preemptive risk mitigation of the sort SB-1047 puts forth is similarly speculative in nature. These are the notions of existential risks developed by Nick Bostrom and Eliezer Yudkowsky. Couched in the parlance of reinforcement learning, these are the kinds of infinite time horizon problems and we have a poor grasp of accurately discounting future rewards (or penalties).

On the other hand, the EU AI Act sets forth a well-bounded set of risk assessment guidelines with examples for each of their hierarchical categories of risky AI systems. Although it is a welcome first step towards ensuring the responsible usage of AI systems, such an approach struggles to address unforeseen risks emerging from rapidly evolving AI capabilities.

At the core of governor Newsom’s rejection is the balance between proactive risk mitigation and fostering responsible innovation. Emphasizing a need to maintain “free-spirited cultivation of intellectual freedom”, Newsom rejects SB-1047's focus on pre-empting unforeseen risks as it could stifle innovation. Finding the right balance between preemptive risk mitigation and reactive risk assessment remains a challenge. There is too much uncertainty with the speed at which AI capability development is achieved and the timeframe of AGI or just what AGI will look like in the future.

[^1]: https://www.wilmerhale.com/en/insights/blogs/wilmerhale-privacy-and-cybersecurity-law/20240717-what-are-highrisk-ai-systems-within-the-meaning-of-the-eus-ai-act-and-what-requirements-apply-to-them
[^2]: https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB1047
